{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed8758b",
   "metadata": {},
   "source": [
    "# Training a Dual Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e936afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffab57e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a552cc0d",
   "metadata": {},
   "source": [
    "## Contrastive Loss using CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81a403f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Demonstrate Contrastive loss using CEL on similarity tensor\n",
    "df = pd.DataFrame(\n",
    "    [\n",
    "        [4.3, 1.2, 0.05, 1.07],\n",
    "        [0.18, 3.2, 0.09, 0.05],\n",
    "        [0.85, 0.27, 2.2, 1.03],\n",
    "        [0.23, 0.57, 0.12, 5.1]\n",
    "    ]\n",
    ")\n",
    "data = torch.tensor(df.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7aff2839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define contrastive loss function\n",
    "def contrastive_loss(data):\n",
    "    target = torch.arange(data.size(0)) #Indexes corresponding to row label\n",
    "    loss = torch.nn.CrossEntropyLoss()(data,target) #Compares index of highest logit to actual\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c02080b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9100, 0.0410, 0.0130, 0.0360],\n",
       "        [0.0429, 0.8801, 0.0393, 0.0377],\n",
       "        [0.1512, 0.0846, 0.5832, 0.1810],\n",
       "        [0.0075, 0.0105, 0.0067, 0.9753]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Softmax(dim = 1)(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e660a58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None diagonal mask:\n",
      "tensor([[False,  True,  True,  True],\n",
      "        [ True, False,  True,  True],\n",
      "        [ True,  True, False,  True],\n",
      "        [ True,  True,  True, False]])\n",
      "tensor([[4.3000, 1.2000, 0.0500, 1.0700],\n",
      "        [0.1800, 3.2000, 0.0900, 0.0500],\n",
      "        [0.8500, 0.2700, 2.2000, 1.0300],\n",
      "        [0.2300, 0.5700, 0.1200, 5.1000]])\n",
      "Loss = 0.19657586514949799\n",
      "tensor([[4.8000, 1.1800, 0.0300, 1.0500],\n",
      "        [0.1600, 3.7000, 0.0700, 0.0300],\n",
      "        [0.8300, 0.2500, 2.7000, 1.0100],\n",
      "        [0.2100, 0.5500, 0.1000, 5.6000]])\n",
      "Loss = 0.12602083384990692\n",
      "tensor([[5.3000, 1.1600, 0.0100, 1.0300],\n",
      "        [0.1400, 4.2000, 0.0500, 0.0100],\n",
      "        [0.8100, 0.2300, 3.2000, 0.9900],\n",
      "        [0.1900, 0.5300, 0.0800, 6.1000]])\n",
      "Loss = 0.07888662070035934\n"
     ]
    }
   ],
   "source": [
    "# Sample optimization loop\n",
    "N = data.size(0)\n",
    "non_diag_mask = ~torch.eye(N, N, dtype=bool)\n",
    "print('None diagonal mask:')\n",
    "print(non_diag_mask)\n",
    "\n",
    "for inx in range(3):\n",
    "    data = torch.tensor(df.values, dtype=torch.float32)\n",
    "    data[range(N), range(N)] += inx*0.5 #Targets diagonal elemenets using interval masking\n",
    "    data[non_diag_mask] -= inx*0.02 #Targets non-diagonal elements using boolean masking\n",
    "    print(data)\n",
    "    print(f\"Loss = {contrastive_loss(data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f540be54",
   "metadata": {},
   "source": [
    "## Encoder Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e15bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self,vocab_size, embed_dim,output_embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, embed_dim) #Creates lookup of embeddings from tokens\n",
    "        self.encoder = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(embed_dim, nhead=8, batch_first=True),\n",
    "            num_layers=3, # Number of Transformer Encoder layers\n",
    "            norm=torch.nn.LayerNorm([embed_dim]), #Normalize along the embedding dimension\n",
    "            enable_nested_tensor=False\n",
    "        )\n",
    "        self.projection = torch.nn.Linear(embed_dim, output_embed_dim)\n",
    "        \n",
    "    def forward(self, tokenizer_output):\n",
    "        x = self.embedding_layer(tokenizer_output['input_ids']) # Returns a sentence matrix\n",
    "        x = self.encoder(x,src_key_padding_mask=tokenizer_output['attention_mask'].logical_not())\n",
    "        cls_embed = x[:,0,:]\n",
    "        return self.projection(cls_embed)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a8e017",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a525f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, num_epochs=10):\n",
    "    embed_size = 512\n",
    "    output_embed_size = 128\n",
    "    max_seq_len = 64\n",
    "    batch_size = 32\n",
    "\n",
    "    n_iters = len(dataset) // batch_size + 1\n",
    "    \n",
    "    # define the question/answer encoders\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    question_encoder = Encoder(tokenizer.vocab_size, embed_size, output_embed_size)\n",
    "    answer_encoder = Encoder(tokenizer.vocab_size, embed_size, output_embed_size)\n",
    "\n",
    "    # define the dataloader, optimizer and loss function    \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)    \n",
    "    optimizer = torch.optim.Adam(list(question_encoder.parameters()) + list(answer_encoder.parameters()), lr=1e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = []\n",
    "        for idx, data_batch in enumerate(dataloader):\n",
    "\n",
    "            # Tokenize the question/answer pairs (each is a batc of 32 questions and 32 answers)\n",
    "            question, answer = data_batch\n",
    "            question_tok = tokenizer(question, padding=True, truncation=True, return_tensors='pt', max_length=max_seq_len)\n",
    "            answer_tok = tokenizer(answer, padding=True, truncation=True, return_tensors='pt', max_length=max_seq_len)\n",
    "            if inx == 0 and epoch == 0:\n",
    "                print(question_tok['input_ids'].shape, answer_tok['input_ids'].shape)\n",
    "            \n",
    "            # Compute the embeddings: the output is of dim = 32 x 128\n",
    "            question_embed = question_encoder(question_tok)\n",
    "            answer_embed = answer_encoder(answer_tok)\n",
    "            if inx == 0 and epoch == 0:\n",
    "                print(question_embed.shape, answer_embed.shape)\n",
    "    \n",
    "            # Compute similarity scores: a 32x32 matrix\n",
    "            # row[N] reflects similarity between question[N] and answers[0...31]\n",
    "            similarity_scores = question_embed @ answer_embed.T\n",
    "            if inx == 0 and epoch == 0:\n",
    "                print(similarity_scores.shape)\n",
    "    \n",
    "            # we want to maximize the values in the diagonal\n",
    "            target = torch.arange(question_embed.shape[0], dtype=torch.long)\n",
    "            loss = loss_fn(similarity_scores, target)\n",
    "            running_loss += [loss.item()]\n",
    "            if idx == n_iters-1:\n",
    "                print(f\"Epoch {epoch}, loss = \", np.mean(running_loss))\n",
    "    \n",
    "            # this is where the magic happens\n",
    "            optimizer.zero_grad()    # reset optimizer so gradients are all-zero\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return question_encoder, answer_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a996781a",
   "metadata": {},
   "source": [
    "## Dataset Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e200ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, datapath):\n",
    "        self.data = pd.read_csv(datapath, sep='\\t', nrows=300)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.data.iloc[idx]['questions'], self.data.iloc[idx]['answers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72ce3bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>who played bubba in the tv series in the heat ...</td>\n",
       "      <td>Carlos Alan Autry Jr. (also known for a period...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>where did the 2017 tour de france start</td>\n",
       "      <td>The 3,540 km (2,200 mi)-long race commenced wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>who is the chess champion of the world</td>\n",
       "      <td>Current world champion Magnus Carlsen won the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>who scored the most hat tricks in football</td>\n",
       "      <td>Cristiano Ronaldo and Messi have scored three ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what do you need to be an ontario scholar</td>\n",
       "      <td>Ontario Scholars are high school graduates in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           questions  \\\n",
       "0  who played bubba in the tv series in the heat ...   \n",
       "1            where did the 2017 tour de france start   \n",
       "2             who is the chess champion of the world   \n",
       "3         who scored the most hat tricks in football   \n",
       "4          what do you need to be an ontario scholar   \n",
       "\n",
       "                                             answers  \n",
       "0  Carlos Alan Autry Jr. (also known for a period...  \n",
       "1  The 3,540 km (2,200 mi)-long race commenced wi...  \n",
       "2  Current world champion Magnus Carlsen won the ...  \n",
       "3  Cristiano Ronaldo and Messi have scored three ...  \n",
       "4  Ontario Scholars are high school graduates in ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = EncoderDataset('nq_sample.tsv')\n",
    "dataset.data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6cc936",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cd0d2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss =  3.810750651359558\n",
      "Epoch 1, loss =  3.5239919662475585\n",
      "Epoch 2, loss =  3.429384684562683\n",
      "Epoch 3, loss =  3.361122727394104\n",
      "Epoch 4, loss =  3.2964673757553102\n"
     ]
    }
   ],
   "source": [
    "Q_encoder, A_encoder = train(dataset, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91765e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2054,  2003,  1996, 13747,  3137,  1999,  1996,  2088,  1029,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "\n",
      "tensor([-0.0242, -0.0792, -0.3418, -0.9653,  0.3829])\n"
     ]
    }
   ],
   "source": [
    "question = 'What is the tallest mountain in the world?'\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "question_tok = tokenizer(question, padding=True, truncation=True, return_tensors='pt', max_length=64)\n",
    "question_emb = Q_encoder(question_tok)[0]\n",
    "print(question_tok)\n",
    "print('\\n')\n",
    "print(question_emb[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "14ad9199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  101,  2054,  2003,  1996, 13747,  3137,  1999,  1996,  2088,  1029,\n",
      "           102]]), tensor([[  101,  1996, 13747,  3137,  1999,  1996,  2088,  2003,  4057, 23914,\n",
      "          1012,   102]]), tensor([[ 101, 2040, 2003, 6221, 9457, 1029,  102]])]\n",
      "tensor([-0.3121,  0.4846, -0.0618, -0.9400, -0.6620])\n",
      "tensor([-0.3959,  0.2701, -0.2616, -1.0928, -0.5686])\n",
      "tensor([-0.8347, -0.0393, -0.3410, -0.4569, -0.3530])\n"
     ]
    }
   ],
   "source": [
    "answers = [\n",
    "    \"What is the tallest mountain in the world?\",\n",
    "    \"The tallest mountain in the world is Mount Everest.\",\n",
    "    \"Who is donald duck?\"\n",
    "]\n",
    "answer_tok = []\n",
    "answer_emb = []\n",
    "for answer in answers:\n",
    "    tok = tokenizer(answer, padding=True, truncation=True, return_tensors='pt', max_length=64)\n",
    "    answer_tok.append(tok['input_ids'])\n",
    "    emb = A_encoder(tok)[0]\n",
    "    answer_emb.append(emb)\n",
    "\n",
    "print(answer_tok)\n",
    "print(answer_emb[0][:5])\n",
    "print(answer_emb[1][:5])\n",
    "print(answer_emb[2][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f680a3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4880, -0.3826,  0.3147])\n"
     ]
    }
   ],
   "source": [
    "# Similarity\n",
    "question_emb @ torch.stack(answer_emb).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bcfbbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
